"""
Smart City Almaty ‚Äî Hybrid AI Engine (Option C)
Runs completely locally, no external APIs required.
Enhanced with GPT-like response generation and 10,000+ word vocabulary.
"""

import re
import json
import random
import os
from typing import Optional, Dict, List, Any
from dataclasses import dataclass, field
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import func
from database import SessionLocal, AIKnowledge

try:
    from enhanced_gpt_ai import get_enhanced_ai, GPTConfig
    HAS_ENHANCED_AI = True
except ImportError:
    HAS_ENHANCED_AI = False

# Import extended databases for GPT-like capabilities
try:
    from conversation_database import (
        VOCABULARY, ECOLOGY_SENTENCES, WEATHER_SENTENCES, 
        TRAFFIC_SENTENCES, SMALL_TALK_SENTENCES, RESPONSE_TEMPLATES,
        MARKOV_PATTERNS, get_random_sentence, build_complex_response,
        get_thinking_phrase, get_transition, get_conclusion, get_empathy
    )
    from extended_dataset import EXTENDED_DATASET, get_extended_dataset
    HAS_EXTENDED_DB = True
except ImportError:
    HAS_EXTENDED_DB = False
    VOCABULARY = {}
    EXTENDED_DATASET = []

# V4.1: LLM runtime is intentionally disabled.
HAS_LLM = False


# ============================================
# ALMATY KNOWLEDGE BASE
# ============================================

ALMATY_KNOWLEDGE = {
    # Emergency Services
    "emergency": {
        "fire": {
            "number": "101",
            "name": "Fire Department",
            "emoji": "üî•",
            "instructions": "State exact address, floor, what is burning, and if there are casualties."
        },
        "police": {
            "number": "102", 
            "name": "Police",
            "emoji": "üëÆ",
            "instructions": "State address, situation description, number and description of suspects."
        },
        "ambulance": {
            "number": "103",
            "name": "Ambulance", 
            "emoji": "üöë",
            "instructions": "State address, age of patient, symptoms, and current condition."
        },
        "gas": {
            "number": "104",
            "name": "Gas Emergency",
            "emoji": "‚ö†Ô∏è",
            "instructions": "Do not turn on lights, open windows, and leave the premises immediately."
        },
        "catastrophe": {
            "number": "112",
            "name": "Unified Rescue Service",
            "emoji": "üÜò",
            "instructions": "Universal number for any emergency or disaster."
        }
    },
    
    # Transport
    "transport": {
        "buses": ["12 (Medeu)", "32", "63", "79 (Airport)", "92 (Airport)", "121", "201 (Tole Bi)"],
        "metro_stations": [
            "Raiymbek Batyr", "Zhibek Zholy", "Almaly", "Abay", 
            "Baikonur", "Auezov Theater", "Alatau", "Sairan",
            "Moskva", "Saryarka", "Dostyk", "Kalkaman (under construction)"
        ],
        "taxi_apps": ["Yandex Go", "inDriver", "Uber"],
        "airport": {
            "name": "Almaty International Airport",
            "code": "ALA",
            "buses_to": ["79", "92", "79E"],
            "location": "Turksib District"
        },
        "railway": {
            "almaty_1": "Northern part of the city, Turksib District",
            "almaty_2": "City center, Ablai Khan Ave"
        }
    },
    
    # City Information
    "city_info": {
        "population": "2.2 million people",
        "area": "682 km¬≤",
        "elevation": "700-900 m above sea level",
        "districts": [
            "Almaly", "Auezov", "Bostandyk", 
            "Zhetysu", "Medeu", "Nauryzbay",
            "Turksib", "Alatau"
        ],
        "landmarks": [
            "Kok-Tobe (TV tower and park)", 
            "Medeu (high-altitude ice rink)", 
            "Shymbulak (ski resort)", 
            "28 Panfilov Heroes Park",
            "Zenkov Cathedral (wooden)", 
            "BAO (Big Almaty Lake)",
            "Arbat (pedestrian zone on Zhibek Zholy)"
        ],
        "education": {
            "universities": ["Al-Farabi KazNU", "KBTU", "Satbayev University", "KIMEP", "IITU"],
            "schools": ["NIS", "RFMSH", "BINOM (upcoming)"]
        }
    },
    
    # Air Quality
    "air_quality": {
        "levels": {
            "good": {"range": "0-50", "emoji": "üü¢", "advice": "Air quality is excellent. Perfect for outdoor activities and sports.", "ru": "–ö–∞—á–µ—Å—Ç–≤–æ –≤–æ–∑–¥—É—Ö–∞ –æ—Ç–ª–∏—á–Ω–æ–µ. –ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Å–ø–æ—Ä—Ç–∞ –∏ –ø—Ä–æ–≥—É–ª–æ–∫."},
            "moderate": {"range": "51-100", "emoji": "üü°", "advice": "Acceptable. Sensitive groups should limit prolonged outdoor exertion.", "ru": "–ü—Ä–∏–µ–º–ª–µ–º–æ. –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º –≥—Ä—É–ø–ø–∞–º —Å—Ç–æ–∏—Ç –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–æ–ª–≥–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —É–ª–∏—Ü–µ."},
            "unhealthy_sensitive": {"range": "101-150", "emoji": "üü†", "advice": "Unhealthy for sensitive groups (children, elderly, asthmatics).", "ru": "–í—Ä–µ–¥–Ω–æ –¥–ª—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä—É–ø–ø."},
            "unhealthy": {"range": "151-200", "emoji": "üî¥", "advice": "Unhealthy for everyone. Windows should be closed, air purifiers recommended.", "ru": "–í—Ä–µ–¥–Ω–æ –¥–ª—è –≤—Å–µ—Ö. –ó–∞–∫—Ä–æ–π—Ç–µ –æ–∫–Ω–∞, –≤–∫–ª—é—á–∏—Ç–µ –æ—á–∏—Å—Ç–∏—Ç–µ–ª–∏ –≤–æ–∑–¥—É—Ö–∞."},
            "very_unhealthy": {"range": "201-300", "emoji": "üü£", "advice": "Very unhealthy. Avoid outdoor physical activity. Wear an N95 mask.", "ru": "–û—á–µ–Ω—å –≤—Ä–µ–¥–Ω–æ. –ò–∑–±–µ–≥–∞–π—Ç–µ —Ñ–∏–∑. –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞ —É–ª–∏—Ü–µ. –ù–æ—Å–∏—Ç–µ –º–∞—Å–∫—É N95."},
            "hazardous": {"range": "300+", "emoji": "üü§", "advice": "Hazardous! Stay indoors. Masks are mandatory.", "ru": "–û–ø–∞—Å–Ω–æ! –û—Å—Ç–∞–≤–∞–π—Ç–µ—Å—å –¥–æ–º–∞. –ú–∞—Å–∫–∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã."}
        }
    },

    # Leiure, Culture & Events (Added)
    "culture": {
        "museums": ["Central State Museum", "Kasteyev State Museum of Arts", "Almaty Museum", "Abay Opera House"],
        "theaters": ["Abay Opera House", "Auezov Theater", "Lermontov Theater", "ARTiSHOCK"],
        "malls": ["Mega Alma-Ata", "Dostyk Plaza", "Esentai Mall", "ADK", "Forum"],
        "parks": ["First President's Park", "Central Park (Gorky)", "Terrenkur", "Botanical Garden"]
    },

    # Healthcare (Added)
    "health": {
        "hospitals": ["Central City Clinical Hospital", "Children's Hospital No. 1", "Emergency Care Center"],
        "private_clinics": ["Dostik Med", "Keruen", "Sema"],
        "emergency_dental": ["Alatau Dental (24/7)", "City Dental Clinic No. 1"]
    }
}

# Intent recognition patterns (English + Russian fallback)
INTENT_PATTERNS = {
    "emergency_fire": [r"fire", r"burning", r"smoke", r"101", r"–ø–æ–∂–∞—Ä", r"–≥–æ—Ä–∏—Ç", r"–¥—ã–º"],
    "emergency_police": [r"police", r"theft", r"robbery", r"attack", r"fight", r"102", r"crime", r"–ø–æ–ª–∏—Ü", r"–∫—Ä–∞–∂–∞"],
    "emergency_ambulance": [r"ambulance", r"doctor", r"pain", r"sick", r"103", r"medical", r"injury", r"heart", r"—Å–∫–æ—Ä–∞—è", r"–≤—Ä–∞—á"],
    "emergency_gas": [r"gas", r"leak", r"104", r"smell of gas", r"–≥–∞–∑", r"—É—Ç–µ—á–∫"],
    "emergency_general": [r"emergency", r"sos", r"ÊïëÂëΩ", r"rescue", r"112", r"—ç–∫—Å—Ç—Ä–µ–Ω", r"—Å—Ä–æ—á–Ω–æ", r"—á–ø", r"–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∞"],
    "transport_bus": [r"bus", r"route", r"stop", r"bus number", r"–∞–≤—Ç–æ–±—É—Å", r"–º–∞—Ä—à—Ä—É—Ç"],
    "transport_metro": [r"metro", r"subway", r"station", r"underground", r"–º–µ—Ç—Ä–æ", r"—Å—Ç–∞–Ω—Ü–∏"],
    "transport_taxi": [r"taxi", r"cab", r"order a car", r"—Ç–∞–∫—Å–∏"],
    "transport_airport": [r"airport", r"plane", r"flight", r"ala", r"–∞—ç—Ä–æ–ø–æ—Ä—Ç"],
    "transport_general": [r"get to", r"reach", r"transport", r"travel", r"–¥–æ–µ—Ö–∞—Ç—å", r"–¥–æ–±—Ä–∞—Ç—å"],
    "weather": [r"weather", r"temperature", r"rain", r"snow", r"cold", r"hot", r"degree", r"forecast", r"–ø–æ–≥–æ–¥", r"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä", r"–≥—Ä–∞–¥—É—Å", r"–æ—Å–∞–¥–∫", r"–¥–æ–∂–¥", r"—Å–Ω–µ–≥"],
    "air_quality": [r"air", r"aqi", r"pollution", r"smog", r"breathe", r"pm2\.?5", r"pm10", r"eco", r"–≤–æ–∑–¥—É—Ö", r"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω", r"—ç–∫–æ–ª–æ–≥–∏—è", r"—Å–º–æ–≥", r"–¥—ã—à–∞—Ç—å"],
    "city_info": [r"almaty", r"city", r"population", r"district", r"landmark", r"university", r"school", r"history", r"fact", r"–∞–ª–º–∞—Ç[—ã—å]", r"–≥–æ—Ä–æ–¥", r"–±–∏–æ–≥—Ä–∞—Ñ–∏—è", r"–∏—Å—Ç–æ—Ä–∏—è", r"—Ä–∞–π–æ–Ω", r"–¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å", r"—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç"],
    "culture": [r"museum", r"theater", r"mall", r"park", r"cinema", r"shopping", r"entertainment", r"leisure", r"–º—É–∑–µ–π", r"—Ç–µ–∞—Ç—Ä", r"–ø–∞—Ä–∫", r"—Ç—Ü", r"—Ä–∞–∑–≤–ª–µ–∫–∞—Ç—å—Å—è", r"–¥–æ—Å—É–≥", r"–∫–∏–Ω–æ"],
    "health": [r"hospital", r"clinic", r"doctor", r"dentist", r"pharmacy", r"medicine", r"–±–æ–ª—å–Ω–∏—Ü–∞", r"–∫–ª–∏–Ω–∏–∫–∞", r"–∞–ø—Ç–µ–∫–∞", r"–≤—Ä–∞—á", r"—Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥", r"–ª–µ—á–∏—Ç—å", r"–±–æ–ª–∏—Ç"],
    "context_query": [r"what did i ask", r"repeat", r"do you remember", r"before", r"earlier", r"—á—Ç–æ —è —Å–ø—Ä–æ—Å–∏–ª", r"–ø–æ–≤—Ç–æ—Ä–∏", r"–ø–æ–º–Ω–∏—à—å"],
    "greeting": [r"hello", r"hi", r"good (morning|afternoon|evening)", r"salem", r"hey", r"–ø—Ä–∏–≤–µ—Ç", r"–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", r"—Å–∞–ª–µ–º", r"–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", r"—É—Ç—Ä–æ"],
    "thanks": [r"thanks", r"thank you", r"appreciate", r"rakhmet", r"—Å–ø–∞—Å–∏–±–æ", r"—Ä–∞—Ö–º–µ—Ç", r"–±–ª–∞–≥–æ–¥–∞—Ä—é"],
    "chat": [r"how are you", r"what's up", r"how's it going", r"who are you", r"you smart", r"love you", r"are you human", r"feelings", r"–∫–∞–∫ –¥–µ–ª–∞", r"–∫–∞–∫ —Ç—ã", r"–∫—Ç–æ —Ç—ã", r"—Ç—ã —á–µ–ª–æ–≤–µ–∫", r"—É–º–Ω—ã–π", r"–ª—é–±–∏—à—å", r"—á–µ –∫–∞–≤–æ", r"–∫–∞–∫ –∂–∏–∑–Ω—å"],
    "help": [r"^help$", r"what can you do", r"functions", r"capabilities", r"—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å", r"^–ø–æ–º–æ—â—å$", r"–ø–æ–º–æ–≥–∏ –º–Ω–µ", r"—É–º–µ–µ—à—å", r"—Ñ—É–Ω–∫—Ü–∏–∏", r"–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏"],
    "infrastructure": [r"node", r"gpu", r"mining", r"reward", r"almt", r"nexus monitor", r"infrastructure", r"–Ω–æ–¥", r"–Ω–∞–≥—Ä–∞–¥–∞", r"–∫—Ä–∏–ø—Ç–∞", r"–∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞", r"–∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å"],
    "neural_shield": [r"shield", r"security", r"bounty", r"bug", r"exploit", r"hack", r"–≤–∑–ª–æ–º", r"–±–∞—É–Ω—Ç–∏", r"—É—è–∑–≤–∏–º–æ—Å—Ç—å", r"—â–∏—Ç", r"–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å"],
    "advice": [r"walk", r"sport", r"outside", r"should i", r"recommend", r"is it good", r"can i", r"–ø—Ä–æ–≥—É–ª–∫–∞", r"–≥—É–ª—è—Ç—å", r"—Å–ø–æ—Ä—Ç", r"—Å–æ–≤–µ—Ç", r"—Å—Ç–æ–∏—Ç –ª–∏", r"—Ä–µ–∫–æ–º–µ–Ω–¥", r"–º–æ–∂–Ω–æ –ª–∏"]
}

@dataclass
class IntentResult:
    intent: str
    confidence: float
    sub_intent: Optional[str] = None
    entities: Dict[str, Any] = None
    language: str = "en"

class IntentClassifier:
    def classify(self, text: str) -> IntentResult:
        text_lower = text.lower()
        best_intent, best_confidence, sub_intent = "unknown", 0.0, None
        
        # Detect language (count characters to be robust against noise)
        ru_chars = len(re.findall(r'[–∞-—è–ê-–Ø—ë–Å]', text))
        en_chars = len(re.findall(r'[a-zA-Z]', text))
        
        # Determine language based on char count
        if ru_chars > 0 and en_chars == 0:
            lang = "ru"
        elif en_chars > 0 and ru_chars == 0:
            lang = "en"
        elif ru_chars > en_chars:
            lang = "ru"
        else:
            lang = "en"

        for intent, patterns in INTENT_PATTERNS.items():
            matches = sum(1 for p in patterns if re.search(p, text_lower))
            if matches > 0:
                conf = min(1.0, matches * 0.3 + 0.4)
                if conf > best_confidence:
                    best_confidence = conf
                    if "_" in intent:
                        parts = intent.split("_", 1)
                        best_intent, sub_intent = parts[0], parts[1]
                    else:
                        best_intent, sub_intent = intent, None
        
        return IntentResult(best_intent, best_confidence, sub_intent, self._extract_entities(text_lower), lang)
    
    def _extract_entities(self, text: str) -> Dict[str, Any]:
        entities = {}
        bus_match = re.search(r'(?:route|bus|number)\s*(\d+)', text)
        if bus_match: entities["bus_number"] = bus_match.group(1)
        
        # Check districts
        for d in ALMATY_KNOWLEDGE["city_info"]["districts"]:
            if d.lower() in text:
                entities["district"] = d
                break
        
        # Check for pronouns (contextual hints)
        if any(w in text for w in ["there", "it", "that", "—Ç–∞–º", "—ç—Ç–æ", "—Ç—É–¥–∞"]):
            entities["has_pronoun"] = True
            
        return entities

class KnowledgeEngine:
    """Deep Research Engine for Almaty Data with GPT-like Generation"""
    def __init__(self):
        self.db_session = SessionLocal()
        self.vocab = self._load_vocabulary()
        self.extended_patterns = self._load_extended_patterns()
        self.conversation_history = []  # For context-aware responses
    
    def _load_vocabulary(self) -> set:
        """Loads common words to filter noise during semantic synthesis"""
        try:
            path = os.path.join(os.path.dirname(__file__), "english_5000.json")
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    return {w.lower() for w in json.load(f)}
        except: pass
        return set()
    
    def _load_extended_patterns(self) -> Dict[str, List[Dict]]:
        """Load extended dataset patterns organized by language"""
        patterns = {"ru": [], "en": []}
        if HAS_EXTENDED_DB and EXTENDED_DATASET:
            for item in EXTENDED_DATASET:
                lang = item.get("language", "en")
                if lang in patterns:
                    patterns[lang].append(item)
        return patterns

    def find_answers(self, query: str, lang: str = "en", limit: int = 3) -> List[Dict[str, Any]]:
        """Multi-fact retrieval for synthesis with extended dataset support"""
        query_words = set(re.findall(r'[\w\d]+', query.lower(), re.UNICODE))
        if not query_words: return []
        
        stop_words = {"the", "is", "at", "which", "on", "in", "for", "a", "an", "and", "or", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–∫–∞–∫", "—ç—Ç–æ", "–º–Ω–µ", "–Ω—É–∂–Ω–æ", "—Ä–∞—Å—Å–∫–∞–∂–∏", "–Ω–∞–π–¥–∏", "can", "you", "tell", "me", "please"}
        query_words = query_words - stop_words
        
        # Expand query words with synonyms
        expanded_query = set(query_words)
        for word in query_words:
            expanded_query.update(self.get_synonyms(word))
        
        results = []
        
        # Search in database
        try:
            knowledge_set = self.db_session.query(AIKnowledge).filter(AIKnowledge.language == lang).all()
            
            for item in knowledge_set:
                pattern_words = set(re.findall(r'[\w\d]+', item.pattern.lower(), re.UNICODE))
                overlap = expanded_query.intersection(pattern_words)
                
                phrase_bonus = 5.0 if item.pattern.lower() in query.lower() else 0.0
                
                keyword_score = 0
                for word in overlap:
                    if word in self.vocab and len(word) < 5:
                        keyword_score += 1
                    else:
                        keyword_score += 4
                
                importance = getattr(item, 'importance', 1)
                score = (keyword_score + phrase_bonus) * (1 + (importance - 1) * 0.3)
                
                if score >= 2.0:
                    results.append({"response": item.response, "score": score, "category": item.category})
        except Exception as e:
            pass  # Continue with extended dataset if DB fails
        
        # Search in extended dataset (in-memory)
        for item in self.extended_patterns.get(lang, []):
            pattern_words = set(re.findall(r'[\w\d]+', item.get("pattern", "").lower(), re.UNICODE))
            overlap = expanded_query.intersection(pattern_words)
            
            if overlap:
                keyword_score = len(overlap) * 3
                phrase_bonus = 5.0 if any(p in query.lower() for p in item.get("pattern", "").split()) else 0.0
                score = keyword_score + phrase_bonus
                
                if score >= 2.0:
                    results.append({"response": item.get("response", ""), "score": score, "category": item.get("category", "GENERAL")})
        
        # Sort by score and return top results
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:limit]

    def find_answer(self, query: str, lang: str = "en") -> Optional[str]:
        """Legacy single-answer retrieval"""
        answers = self.find_answers(query, lang, limit=1)
        return answers[0]["response"] if answers else None
    
    def generate_contextual_response(self, query: str, lang: str = "ru", topic: str = "general") -> str:
        """GPT-like response generation using extended vocabulary and templates"""
        if not HAS_EXTENDED_DB:
            return None
        
        # Build complex response using Markov-style patterns
        return build_complex_response(topic, lang, num_facts=random.randint(2, 4))
    
    def get_topic_sentences(self, topic: str, lang: str = "ru", count: int = 3) -> List[str]:
        """Get relevant sentences for a topic"""
        if not HAS_EXTENDED_DB:
            return []
        
        topic_map = {
            "ecology": ECOLOGY_SENTENCES,
            "weather": WEATHER_SENTENCES, 
            "traffic": TRAFFIC_SENTENCES,
            "general": SMALL_TALK_SENTENCES
        }
        
        sentences_dict = topic_map.get(topic, SMALL_TALK_SENTENCES)
        sentences = sentences_dict.get(lang, sentences_dict.get("ru", []))
        
        if sentences:
            return random.sample(sentences, min(count, len(sentences)))
        return []
    
    def enrich_response(self, base_response: str, topic: str, lang: str = "ru") -> str:
        """
        Returns response as-is. 
        Removed random elaborations for cleaner, more predictable output.
        """
        return base_response

    def get_synonyms(self, word: str) -> List[str]:
        # Expanded local synonym map with more terms
        syns = {
            "–¥–æ—Å—É–≥": ["–æ—Ç–¥—ã—Ö", "—Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è", "—Å—Ö–æ–¥–∏—Ç—å", "–ø–æ–≥—É–ª—è—Ç—å", "–º–µ—Å—Ç–∞", "fun", "relax"],
            "–ª–µ—á–∏—Ç—å": ["–±–æ–ª—å–Ω–∏—Ü–∞", "–∫–ª–∏–Ω–∏–∫–∞", "–≤—Ä–∞—á", "–ø–æ–º–æ—â—å", "–∑–¥–æ—Ä–æ–≤—å–µ", "–º–µ–¥–∏—Ü–∏–Ω–∞", "health"],
            "—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç": ["–∞–≤—Ç–æ–±—É—Å", "–º–µ—Ç—Ä–æ", "—Ç–∞–∫—Å–∏", "–µ—Ö–∞—Ç—å", "–º–∞—Ä—à—Ä—É—Ç", "–¥–æ—Ä–æ–≥–∞", "bus", "metro"],
            "ecology": ["air", "pollution", "smog", "aqi", "environment", "smoke", "—ç–∫–æ–ª–æ–≥–∏—è", "–≤–æ–∑–¥—É—Ö"],
            "weather": ["–ø–æ–≥–æ–¥–∞", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥", "climate", "forecast"],
            "traffic": ["–ø—Ä–æ–±–∫–∞", "–∑–∞—Ç–æ—Ä", "–¥–≤–∏–∂–µ–Ω–∏–µ", "–¥–æ—Ä–æ–≥–∞", "congestion", "jam"],
            "history": ["past", "origin", "founded", "old", "ancient", "–∏—Å—Ç–æ—Ä–∏—è", "–ø—Ä–æ—à–ª–æ–µ"],
            "sights": ["landmarks", "places", "visit", "see", "monument", "–¥–æ—Å—Ç–æ–ø—Ä–∏–º–µ—á–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"],
            "–µ–¥–∞": ["–∫—É—Ö–Ω—è", "—Ä–µ—Å—Ç–æ—Ä–∞–Ω", "–∫–∞—Ñ–µ", "—Ç—Ä–∞–¥–∏—Ü–∏–∏", "–±–ª—é–¥–∞", "food", "eat"],
            "—Å–ø–æ—Ä—Ç": ["–ª—ã–∂–∏", "–∫–æ–Ω—å–∫–∏", "–º–µ–¥–µ–æ", "—à—ã–º–±—É–ª–∞–∫", "—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞", "sport", "ski"],
            "—Ä–∞–±–æ—Ç–∞": ["–∫–∞—Ä—å–µ—Ä–∞", "–≤–∞–∫–∞–Ω—Å–∏–∏", "job", "career", "work"],
            "—É—á–µ–±–∞": ["–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "—à–∫–æ–ª–∞", "education", "study"],
            "–ø—Ä–æ–±–∫–∏": ["–∑–∞—Ç–æ—Ä", "—Ç—Ä–∞—Ñ–∏–∫", "–¥–≤–∏–∂–µ–Ω–∏–µ", "traffic", "congestion", "jam"],
            "—Å–º–æ–≥": ["–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ", "–≤–æ–∑–¥—É—Ö", "—ç–∫–æ–ª–æ–≥–∏—è", "pollution", "smog", "air quality"],
        }
        return syns.get(word.lower(), [])

    def close(self):
        self.db_session.close()


class ResponseGenerator:
    def __init__(self, memory):
        self.classifier = IntentClassifier()
        self.knowledge = KnowledgeEngine()
        self.memory = memory
    
    def generate(self, message: str, context: Optional[Dict] = None) -> str:
        """
        Modified generation flow:
        Use the new EnhancedGPTStyleAI if available, 
        otherwise fall back to old pattern matching.
        """
        if HAS_ENHANCED_AI:
            ai = get_enhanced_ai()
            # We can pass session_id if we have it in context, else 'default'
            session_id = context.get('session_id', 'default') if context else 'default'
            return ai.chat(message, session_id=session_id)

        # --- LEGACY FLOW BELOW ---
        result = self.classifier.classify(message)
        result.entities["raw_text"] = message

        # 0. Contextual Enrichment
        if result.entities.get("has_pronoun"):
            last_intent = self.memory.get_last_intent()
            if last_intent and result.intent == "unknown":
                result.intent = last_intent
        
        # Handler mapping
        handlers = {
            "emergency": self._handle_emergency, "transport": self._handle_transport,
            "weather": self._handle_weather, "air": self._handle_air_quality,
            "city": self._handle_city_info, "context": self._handle_context,
            "culture": self._handle_culture, "health": self._handle_health,
            "infrastructure": self._handle_infrastructure, "neural_shield": self._handle_shield,
            "advice": self._handle_advice, "chat": self._handle_chat_intent,
            "greeting": self._handle_greeting, "thanks": self._handle_thanks,
            "help": self._handle_help, "unknown": self._handle_unknown,
        }
        
        # 1. Handle simple intents directly (no DB search needed)
        simple_intents = {"greeting", "thanks", "help", "emergency"}
        if result.intent in simple_intents:
            handler_response = handlers[result.intent](result, context or {})
            return self._apply_reasoning(handler_response, context or {}, result.language)
        
        # 2. For other intents, try specialized handlers first
        if result.intent in handlers and result.intent != "unknown":
            handler_response = handlers[result.intent](result, context or {})
            return self._apply_reasoning(handler_response, context or {}, result.language)
        
        # 3. For unknown intents - search knowledge base
        facts = self.knowledge.find_answers(message, result.language, limit=2)
        if facts:
            response = self._synthesize_facts(facts, result.language)
            return self._apply_reasoning(response, context or {}, result.language)

        # 4. Final fallback
        return self._apply_reasoning(
            self._handle_unknown(result, context or {}), 
            context or {}, 
            result.language
        )

    def _simulate_cot(self, intent: str, lang: str) -> str:
        """
        Generates a brief 'thought process' string.
        Now shows only 5% of the time to keep responses clean and concise.
        """
        # Show thinking process only rarely (5% of cases) for cleaner responses
        if random.random() > 0.05:
            return ""
        
        is_ru = lang == "ru"
        
        # Short, focused thinking indicators
        if intent in ["emergency", "emergency_fire", "emergency_police", "emergency_ambulance"]:
            return ""  # Never show thinking for emergencies - direct response needed
        
        if is_ru:
            return "> üîç *–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å...*\n\n"
        return "> üîç *Processing...*\n\n"

    def _handle_chat_intent(self, result, context):
        """Dedicated handler for the new conversational knowledge base"""
        # Prioritize matching from the expanded dataset
        facts = self.knowledge.find_answers(result.entities.get("raw_text", ""), result.language, limit=1)
        if facts:
            return facts[0]["response"]
        
        # Fallback to a generic polite response
        if result.language == "ru":
            return "–Ø –≤—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤ –ø–æ–æ–±—â–∞—Ç—å—Å—è! –†–∞—Å—Å–∫–∞–∂–∏—Ç–µ, —á—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç –≤ –ê–ª–º–∞—Ç—ã?"
        return "I'm always here to talk! Tell me, what interests you about Almaty today?"

    def _synthesize_facts(self, facts: List[Dict], lang: str) -> str:
        """
        Combines multiple facts into a clean, concise response.
        Simplified to avoid verbose introductions and transitions.
        """
        is_ru = lang == "ru"
        
        # Deduplicate and format
        responses = []
        for f in facts:
            resp = f["response"].strip()
            if resp not in responses:
                responses.append(resp)
        
        if not responses: 
            return ""
        
        # Single fact - return as is
        if len(responses) == 1: 
            return responses[0]
        
        # Multiple facts - combine cleanly with simple separator
        # Limit to 2 facts max for brevity
        responses = responses[:2]
        
        if is_ru:
            return f"{responses[0]}\n\n{responses[1]}" if len(responses) > 1 else responses[0]
        else:
            return f"{responses[0]}\n\n{responses[1]}" if len(responses) > 1 else responses[0]

    def _apply_reasoning(self, response: str, context: Dict, lang: str) -> str:
        """
        Apply contextual reasoning only when truly relevant.
        Simplified to avoid random distractions and verbose prefixes.
        """
        air = context.get("air", {})
        aqi = air.get("aqi", 0)
        weather = context.get("weather", {})
        temp = weather.get("temperature")
        is_ru = lang == "ru"
        
        reasoning = []
        
        # Only add contextual warnings when genuinely important (high thresholds)
        if aqi > 150:  # Only for actually unhealthy air
            if is_ru: 
                reasoning.append(f"‚ö†Ô∏è AQI —Å–µ–π—á–∞—Å {aqi} ‚Äî –æ–≥—Ä–∞–Ω–∏—á—å—Ç–µ –≤—Ä–µ–º—è –Ω–∞ —É–ª–∏—Ü–µ.")
            else: 
                reasoning.append(f"‚ö†Ô∏è AQI is {aqi} ‚Äî limit outdoor time.")
        
        # Weather extremes only
        if temp is not None:
            if temp > 38:
                reasoning.append("üî• –ë–µ—Ä–µ–≥–∏—Ç–µ—Å—å –∂–∞—Ä—ã!" if is_ru else "üî• Stay cool!")
            elif temp < -20:
                reasoning.append("‚ùÑÔ∏è –°–∏–ª—å–Ω—ã–π –º–æ—Ä–æ–∑!" if is_ru else "‚ùÑÔ∏è Extreme cold!")

        # Build final response without verbose prefix
        suffix = ""
        if reasoning:
            suffix = "\n\n" + " ‚Ä¢ ".join(reasoning)
        
        return f"{response}{suffix}"
    
    def _handle_context(self, result, context):
        history = self.memory.get_context()
        past_user_msgs = [m["content"] for m in history[:-1] if m["role"] == "user"]
        if not past_user_msgs: return "We just started our conversation! I don't have anything to recall yet. üòä"
        return f"You recently asked: *\"{past_user_msgs[-1]}\"*.\n\nI remember our conversation and am ready to continue!"

    def _handle_emergency(self, result, context):
        em = ALMATY_KNOWLEDGE["emergency"]
        is_ru = result.language == "ru"
        
        title = "üìû **–≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–ª—É–∂–±—ã –ê–ª–º–∞—Ç—ã:**" if is_ru else "üìû **Almaty Emergency Services:**"
        footer = "\n\nüöë *–í –ª—é–±–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –∑–≤–æ–Ω–∏—Ç–µ 112*" if is_ru else "\n\nüöë *In any situation, you can call 112*"

        if result.sub_intent in em:
            svc = em[result.sub_intent]
            # Simple localization for keys if needed
            name = svc['name']
            if is_ru:
                names = {"Fire Department": "–ü–æ–∂–∞—Ä–Ω–∞—è —Å–ª—É–∂–±–∞", "Police": "–ü–æ–ª–∏—Ü–∏—è", "Ambulance": "–°–∫–æ—Ä–∞—è –ø–æ–º–æ—â—å", "Gas Emergency": "–ê–≤–∞—Ä–∏–π–Ω–∞—è —Å–ª—É–∂–±–∞ –≥–∞–∑–∞", "Unified Rescue Service": "–°–ª—É–∂–±–∞ —Å–ø–∞—Å–µ–Ω–∏—è 112"}
                name = names.get(name, name)
            return f"{svc['emoji']} **{name}**: {svc['number']}\n\n{svc['instructions'] if not is_ru else '–í—ã–∑—ã–≤–∞–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.'}"
            
        lines = [title]
        for svc in em.values(): 
            name = svc['name']
            if is_ru:
                names = {"Fire Department": "–ü–æ–∂–∞—Ä–Ω–∞—è", "Police": "–ü–æ–ª–∏—Ü–∏—è", "Ambulance": "–°–∫–æ—Ä–∞—è", "Gas Emergency": "–ì–∞–∑", "Unified Rescue Service": "–ß–°/112"}
                name = names.get(name, name)
            lines.append(f"{svc['emoji']} {name}: **{svc['number']}**")
        return "\n".join(lines) + footer

    def _handle_transport(self, result, context):
        tr = ALMATY_KNOWLEDGE["transport"]
        is_ru = result.language == "ru"
        
        if result.sub_intent == "metro":
            msg = f"üöá **Almaty Metro**\n\nStations: {', '.join(tr['metro_stations'][:8])}...\n\n‚è∞ Operating hours: 06:20 - 00:00"
            if is_ru:
                msg = f"üöá **–ê–ª–º–∞—Ç–∏–Ω—Å–∫–∏–π –º–µ—Ç—Ä–æ–ø–æ–ª–∏—Ç–µ–Ω**\n\n–°—Ç–∞–Ω—Ü–∏–∏: {', '.join(tr['metro_stations'][:8])}...\n\n‚è∞ –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: 06:20 - 00:00"
            return msg
            
        if result.sub_intent == "airport":
            msg = f"‚úàÔ∏è **{tr['airport']['name']}**\n\nüöå Buses: {', '.join(tr['airport']['buses_to'])}\nüìç {tr['airport']['location']}"
            if is_ru:
                msg = f"‚úàÔ∏è **–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π –∞—ç—Ä–æ–ø–æ—Ä—Ç –ê–ª–º–∞—Ç—ã**\n\nüöå –ê–≤—Ç–æ–±—É—Å—ã: {', '.join(tr['airport']['buses_to'])}\nüìç –¢—É—Ä–∫—Å–∏–±—Å–∫–∏–π —Ä–∞–π–æ–Ω"
            return msg
            
        if result.sub_intent == "bus" or result.entities.get("bus_number"):
            num = result.entities.get("bus_number", "")
            if num:
                if is_ru: return f"üöå –ê–≤—Ç–æ–±—É—Å **{num}**: –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–ª–µ–º–µ—Ç—Ä–∏–∏... –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è CityBus –∏–ª–∏ Onay."
                return f"üöå Bus **{num}**: Loading telemetry... For real-time tracking, use CityBus or Onay app."
            
            if is_ru: return f"üöå **–û—Å–Ω–æ–≤–Ω—ã–µ –º–∞—Ä—à—Ä—É—Ç—ã**: {', '.join(tr['buses'])}\n\nüí° –ú–∞—Ä—à—Ä—É—Ç ‚Ññ12 –µ–¥–µ—Ç –¥–æ –ú–µ–¥–µ—É!"
            return f"üöå **Main Bus Routes**: {', '.join(tr['buses'])}\n\nüí° Route #12 goes to Medeu!"
            
        if is_ru: return "üöå **–¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç –ê–ª–º–∞—Ç—ã**: –ú–µ—Ç—Ä–æ, 150+ –∞–≤—Ç–æ–±—É—Å–Ω—ã—Ö –º–∞—Ä—à—Ä—É—Ç–æ–≤, —Å–∏—Å—Ç–µ–º–∞ –æ–ø–ª–∞—Ç—ã Onay. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 2GIS –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–æ–≤."
        return "üöå **Almaty Transport**: Metro, 100+ bus routes, Onay payment system. Use 2GIS for optimal routing."

    def _handle_weather(self, result, context):
        w = context.get("weather", {})
        is_ru = result.language == "ru"
        if w:
            temp = w.get('temperature', 'N/A')
            desc = w.get('description', 'clear')
            hum = w.get('humidity', 'N/A')
            if is_ru:
                return f"üå§Ô∏è **–ü–æ–≥–æ–¥–∞**: **{temp}¬∞C**, {desc}. üíß –í–ª–∞–∂–Ω–æ—Å—Ç—å: {hum}%"
            return f"üå§Ô∏è **Weather**: **{temp}¬∞C**, {desc}. üíß Humidity: {hum}%"
        if is_ru:
            return "üå§Ô∏è –ü–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å –∫–æ–º—Ñ–æ—Ä—Ç–Ω–∞—è. –î–∞–Ω–Ω—ã–µ —Å–µ–Ω—Å–æ—Ä–æ–≤ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        return "üå§Ô∏è The weather is comfortable right now. Sensor data is temporarily unavailable."

    def _handle_air_quality(self, result, context):
        air = context.get("air", {})
        aqi = air.get("aqi", 0)
        is_ru = result.language == "ru"
        levels = ALMATY_KNOWLEDGE["air_quality"]["levels"]
        lv = levels["good"] if aqi <= 50 else levels["moderate"] if aqi <= 100 else levels["unhealthy_sensitive"] if aqi <= 150 else levels["unhealthy"] if aqi <= 200 else levels["very_unhealthy"] if aqi <= 300 else levels["hazardous"]
        
        advice = lv["ru"] if is_ru else lv["advice"]
        return f"üí® **Air Quality**: AQI **{aqi}**, PM2.5: {air.get('pm25', 'N/A')} ¬µg/m¬≥\n\n{lv['emoji']} {advice}"

    def _handle_culture(self, result, context):
        cu = ALMATY_KNOWLEDGE["culture"]
        is_ru = result.language == "ru"
        if is_ru:
            return f"üé≠ **–ö—É–ª—å—Ç—É—Ä–∞ –∏ –î–æ—Å—É–≥**:\n\nüèõÔ∏è **–ú—É–∑–µ–∏**: {', '.join(cu['museums'][:3])}\nüõçÔ∏è **–¢–¶**: {', '.join(cu['malls'][:3])}\nüå≥ **–ü–∞—Ä–∫–∏**: {', '.join(cu['parks'][:3])}\n\n–ß—Ç–æ –∏–º–µ–Ω–Ω–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç?"
        return f"üé≠ **Culture & Leisure**:\n\nüèõÔ∏è **Museums**: {', '.join(cu['museums'][:3])}\nüõçÔ∏è **Shopping**: {', '.join(cu['malls'][:3])}\nüå≥ **Parks**: {', '.join(cu['parks'][:3])}\n\nWhat would you like to explore?"

    def _handle_health(self, result, context):
        he = ALMATY_KNOWLEDGE["health"]
        is_ru = result.language == "ru"
        if is_ru:
            return f"üè• **–ó–¥–æ—Ä–æ–≤—å–µ –∏ –º–µ–¥–∏—Ü–∏–Ω–∞**:\n\nüöë **–ì–æ—Å–ø–∏—Ç–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—Ç—Ä—ã**: {', '.join(he['hospitals'])}\nü¶∑ **–°—Ç–æ–º–∞—Ç–æ–ª–æ–≥–∏—è 24/7**: {he['emergency_dental'][0]}\n\n–ë–µ—Ä–µ–≥–∏—Ç–µ —Å–µ–±—è! –ï—Å–ª–∏ —ç—Ç–æ —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π —Å–ª—É—á–∞–π, –∑–≤–æ–Ω–∏—Ç–µ **103**."
        return f"üè• **Healthcare & Medical**:\n\nüöë **Major Hospitals**: {', '.join(he['hospitals'])}\nü¶∑ **24/7 Dental**: {he['emergency_dental'][0]}\n\nStay safe! If this is an emergency, call **103**."

    def _handle_infrastructure(self, result, context):
        is_ru = result.language == "ru"
        if is_ru:
            return "üèóÔ∏è **–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –Ω–æ–¥—ã**: –í—ã –º–æ–∂–µ—Ç–µ —Å–¥–∞–≤–∞—Ç—å –≤ –∞—Ä–µ–Ω–¥—É —Å–≤–æ–∏ GPU –º–æ—â–Ω–æ—Å—Ç–∏ –∏–ª–∏ –¥–µ–ª–∏—Ç—å—Å—è –¥–∞–Ω–Ω—ã–º–∏ –¥–∞—Ç—á–∏–∫–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ **Infrastructure**. –ó–∞ —ç—Ç–æ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ —Ç–æ–∫–µ–Ω—ã **ALMT**."
        return "üèóÔ∏è **Infrastructure Nodes**: You can lease your GPU power or share sensor data on the **Infrastructure** page. This earns you **ALMT** tokens for contributing to the city's AI core."

    def _handle_shield(self, result, context):
        is_ru = result.language == "ru"
        if is_ru:
            return "üõ°Ô∏è **Neural Shield**: –ù–∞—à–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞ Bug Bounty. –ù–∞–π–¥–∏—Ç–µ —É—è–∑–≤–∏–º–æ—Å—Ç—å –≤ –≥–æ—Ä–æ–¥—Å–∫–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –∏ –ø–æ–ª—É—á–∏—Ç–µ –Ω–∞–≥—Ä–∞–¥—É –¥–æ $1,000. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã."
        return "üõ°Ô∏è **Neural Shield**: Our Bug Bounty program. Find vulnerabilities in city systems and claim rewards up to $1,000. Details are in the Infrastructure tab."

    def _handle_advice(self, result, context):
        air = context.get("air", {})
        aqi = air.get("aqi", 0)
        weather = context.get("weather", {})
        temp = weather.get("temperature", 20)
        is_ru = result.language == "ru"
        
        if aqi > 150:
            if is_ru: return "üö´ **–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é**: –ö–∞—á–µ—Å—Ç–≤–æ –≤–æ–∑–¥—É—Ö–∞ —Å–µ–π—á–∞—Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ (AQI {aqi}). –õ—É—á—à–µ –æ—Å—Ç–∞—Ç—å—Å—è –¥–æ–º–∞."
            return f"üö´ **Not Recommended**: Air quality is critical (AQI {aqi}) right now. Better stay indoors."
        
        if temp < -15:
            if is_ru: return f"‚ùÑÔ∏è **–•–æ–ª–æ–¥–Ω–æ**: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ {temp}¬∞C. –ï—Å–ª–∏ –≤—ã–π–¥–µ—Ç–µ, –æ–¥–µ–≤–∞–π—Ç–µ—Å—å –æ—á–µ–Ω—å —Ç–µ–ø–ª–æ. –ù–æ –≤–æ–∑–¥—É—Ö —Å–µ–≥–æ–¥–Ω—è —á–∏—Å—Ç—ã–π!"
            return f"‚ùÑÔ∏è **Very Cold**: It's {temp}¬∞C. Dress very warmly if you go out. Air is clear though!"
            
        if is_ru: return "‚úÖ **–û—Ç–ª–∏—á–Ω–æ–µ –≤—Ä–µ–º—è**: –ü–æ–≥–æ–¥–∞ –∏ –≤–æ–∑–¥—É—Ö –≤ –Ω–æ—Ä–º–µ. –ü—Ä–æ–≥—É–ª–∫–∞ –∏–ª–∏ —Å–ø–æ—Ä—Ç –Ω–∞ —Å–≤–µ–∂–µ–º –≤–æ–∑–¥—É—Ö–µ –ø–æ–π–¥—É—Ç –Ω–∞ –ø–æ–ª—å–∑—É!"
        return "‚úÖ **Great time**: Both weather and air quality are within normal limits. A walk or outdoor sport would be beneficial!"

    def _handle_city_info(self, result, context):
        ci = ALMATY_KNOWLEDGE["city_info"]
        is_ru = result.language == "ru"
        if is_ru:
            return f"üèôÔ∏è **–ê–ª–º–∞—Ç—ã** (–Æ–∂–Ω–∞—è —Å—Ç–æ–ª–∏—Ü–∞)\n\nüë• –ù–∞—Å–µ–ª–µ–Ω–∏–µ: 2.2 –º–∏–ª–ª–∏–æ–Ω–∞ —á–µ–ª–æ–≤–µ–∫\nüìç **–ú–µ—Å—Ç–∞**: –ö–æ–∫-–¢–æ–±–µ, –ú–µ–¥–µ—É, –®—ã–º–±—É–ª–∞–∫, –ü–∞—Ä–∫ 28 –ø–∞–Ω—Ñ–∏–ª–æ–≤—Ü–µ–≤\nüèõÔ∏è **–†–∞–π–æ–Ω—ã**: {', '.join(['–ê–ª–º–∞–ª–∏–Ω—Å–∫–∏–π', '–ú–µ–¥–µ—É—Å–∫–∏–π', '–ê—É—ç–∑–æ–≤—Å–∫–∏–π', '–ë–æ—Å—Ç–∞–Ω–¥—ã–∫—Å–∫–∏–π'])}"
        return f"üèôÔ∏è **Almaty** (The Southern Capital)\n\nüë• Population: {ci['population']}\nüìç **Landmarks**: {', '.join(ci['landmarks'][:5])}\nüèõÔ∏è **Districts**: {', '.join(ci['districts'])}"

    def _handle_greeting(self, result, context):
        if result.language == "ru":
            greetings = [
                "üëã –ü—Ä–∏–≤–µ—Ç! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?",
                "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –°–ø—Ä–∞—à–∏–≤–∞–π—Ç–µ –æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–µ, –ø–æ–≥–æ–¥–µ –∏–ª–∏ –≥–æ—Ä–æ–¥–µ.",
                "–°–∞–ª–µ–º! –Ø –≤–∞—à –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –ê–ª–º–∞—Ç—ã."
            ]
            return random.choice(greetings)
        greetings = [
            "üëã Hello! How can I help?",
            "Hi there! Ask me about transport, weather, or the city.",
            "Hey! I'm your Almaty assistant."
        ]
        return random.choice(greetings)

    def _handle_thanks(self, result, context):
        if result.language == "ru":
            return random.choice(["üòä –ü–æ–∂–∞–ª—É–π—Å—Ç–∞!", "–†–∞–¥ –ø–æ–º–æ—á—å! üèîÔ∏è", "–ë–µ–∑ –ø—Ä–æ–±–ª–µ–º! üá∞üáø"])
        return random.choice(["üòä You're welcome!", "Happy to help! üèîÔ∏è", "No problem! üá∞üáø"])

    def _handle_help(self, result, context):
        if result.language == "ru":
            return "–ú–æ–≥—É –ø–æ–º–æ—á—å —Å:\n‚Ä¢ üö® –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ (101, 102, 103, 112)\n‚Ä¢ üöå –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç\n‚Ä¢ üí® –ö–∞—á–µ—Å—Ç–≤–æ –≤–æ–∑–¥—É—Ö–∞\n‚Ä¢ üå§Ô∏è –ü–æ–≥–æ–¥–∞\n‚Ä¢ üèôÔ∏è –ì–æ—Ä–æ–¥"
        return "I can help with:\n‚Ä¢ üö® Emergency (101, 102, 103, 112)\n‚Ä¢ üöå Transport\n‚Ä¢ üí® Air quality\n‚Ä¢ üå§Ô∏è Weather\n‚Ä¢ üèôÔ∏è City info"

    def _handle_unknown(self, result, context):
        """
        Handle unknown intents with concise, helpful fallback.
        """
        is_ru = result.language == "ru"
        raw_text = result.entities.get("raw_text", "")
        
        # Try to find keyword matches in KnowledgeEngine
        facts = self.knowledge.find_answers(raw_text, result.language, limit=2)
        if facts:
            return self._synthesize_facts(facts, result.language)
        
        # Short query clarification
        words = raw_text.split()
        if len(words) <= 1:
            if is_ru: 
                return "–û —á—ë–º –≤—ã —Ö–æ—Ç–∏—Ç–µ —É–∑–Ω–∞—Ç—å? –°–ø—Ä–æ—Å–∏—Ç–µ –ø—Ä–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç, –ø–æ–≥–æ–¥—É, —ç–∫–æ–ª–æ–≥–∏—é –∏–ª–∏ –≥–æ—Ä–æ–¥."
            return "What would you like to know? Ask about transport, weather, ecology, or the city."

        # Simple fallback
        if is_ru:
            return "–ù–µ –Ω–∞—à—ë–ª —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–ø—Ä–æ—Å–∏—Ç—å –ø—Ä–æ:\n‚Ä¢ –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç (–º–µ—Ç—Ä–æ, –∞–≤—Ç–æ–±—É—Å—ã)\n‚Ä¢ –ü–æ–≥–æ–¥—É\n‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –≤–æ–∑–¥—É—Ö–∞\n‚Ä¢ –≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–µ —Å–ª—É–∂–±—ã (101, 102, 103)"
        else:
            return "I couldn't find specific info. Try asking about:\n‚Ä¢ Transport (metro, buses)\n‚Ä¢ Weather\n‚Ä¢ Air quality\n‚Ä¢ Emergency services (101, 102, 103)"
    
    def _detect_topic(self, text: str) -> str:
        """Detect the main topic from user input for contextual response generation"""
        text_lower = text.lower()
        
        topic_keywords = {
            "ecology": ["–≤–æ–∑–¥—É—Ö", "—ç–∫–æ–ª–æ–≥–∏—è", "—Å–º–æ–≥", "–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ", "aqi", "pm2.5", "air", "pollution", "smog", "environment"],
            "weather": ["–ø–æ–≥–æ–¥–∞", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥", "–ø—Ä–æ–≥–Ω–æ–∑", "weather", "temperature", "rain", "snow", "forecast", "climate"],
            "traffic": ["–ø—Ä–æ–±–∫–∞", "–∑–∞—Ç–æ—Ä", "—Ç—Ä–∞—Ñ–∏–∫", "–¥–æ—Ä–æ–≥–∞", "–¥–≤–∏–∂–µ–Ω–∏–µ", "traffic", "jam", "congestion", "road", "driving"],
        }
        
        max_matches = 0
        detected_topic = "general"
        
        for topic, keywords in topic_keywords.items():
            matches = sum(1 for kw in keywords if kw in text_lower)
            if matches > max_matches:
                max_matches = matches
                detected_topic = topic
        
        return detected_topic

class ConversationMemory:
    def __init__(self, max_history=10):
        self.history = []
        self.max_history = max_history
        self.last_intent = None

    def add(self, role, content, intent=None):
        self.history.append({
            "role": role, 
            "content": content, 
            "intent": intent, 
            "timestamp": datetime.now().isoformat()
        })
        if intent: self.last_intent = intent
        if len(self.history) > self.max_history: self.history.pop(0)

    def get_context(self): return self.history.copy()
    def get_last_intent(self): return self.last_intent
    def clear(self): 
        self.history = []
        self.last_intent = None

class SmartCityAI:
    def __init__(self):
        self.memory = ConversationMemory()
        self.generator = ResponseGenerator(self.memory)
    def chat(self, message, context=None):
        if HAS_ENHANCED_AI:
            # Use Enhanced AI for chat as well
            ai = get_enhanced_ai()
            # Ensure history is synced if needed, but enhanced_ai has its own history
            response = ai.chat(message, session_id="mem_session")
            # Keep sync with legacy memory for history endpoints
            self.memory.add("user", message)
            self.memory.add("assistant", response)
            return response

        # Legacy chat
        res = self.generator.generate(message, context)
        self.memory.add("user", message, "unknown")
        self.memory.add("assistant", res)
        return res
    def get_history(self): return self.memory.get_context()
    def clear_history(self): self.memory.clear()

_ai_instance = None
def get_ai():
    global _ai_instance
    if _ai_instance is None: _ai_instance = SmartCityAI()
    return _ai_instance
